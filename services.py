import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
from langchain_huggingface import HuggingFacePipeline
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from huggingface_hub import login
# from langchain_community.llms import VLLM  # B·ªè vLLM
import pandas as pd
import psycopg2
from psycopg2.extras import RealDictCursor
from sqlalchemy import create_engine, text
import json

# Import c·∫•u h√¨nh t·ª´ file config.py
import config

def login_huggingface():
    """ƒêƒÉng nh·∫≠p v√†o Hugging Face."""
    if config.HUGGINGFACE_TOKEN:
        login(token=config.HUGGINGFACE_TOKEN)
        print("‚úÖ ƒê√£ ƒëƒÉng nh·∫≠p Hugging Face!")
    else:
        print("‚ö†Ô∏è C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y HUGGINGFACE_ACCESS_TOKEN.")

# def load_llm_pipeline():
#     """
#     T·∫£i m√¥ h√¨nh LLM (4-bit) v√† t·∫°o ra HuggingFacePipeline c·ªßa LangChain.
#     """
#     print(f"B·∫Øt ƒë·∫ßu t·∫£i m√¥ h√¨nh: {config.LLM_MODEL_NAME} (ch·∫ø ƒë·ªô 4-bit)")
    
#     quantization_config = BitsAndBytesConfig(
#         load_in_4bit=True,
#         bnb_4bit_compute_dtype=torch.bfloat16,
#     )

#     tokenizer = AutoTokenizer.from_pretrained(
#         config.LLM_MODEL_NAME,
#         cache_dir=config.MODEL_CACHE_DIR,
#     )

#     model = AutoModelForCausalLM.from_pretrained(
#         config.LLM_MODEL_NAME,
#         quantization_config=quantization_config,
#         device_map="auto",
#         cache_dir=config.MODEL_CACHE_DIR
#     )
    
#     print("‚úÖ T·∫£i m√¥ h√¨nh LLM th√†nh c√¥ng (ch·∫ø ƒë·ªô 4-bit).")

#     text_generator = pipeline(
#         "text-generation",
#         model=model,
#         tokenizer=tokenizer,
#         max_new_tokens=512,
#         do_sample=True,
#         temperature=0.1,
#         return_full_text=False
#     )
    
#     return HuggingFacePipeline(pipeline=text_generator)

def load_llm_pipeline():
    """
    T·∫£i m√¥ h√¨nh LLM b·∫±ng Transformers Pipeline (kh√¥ng c·∫ßn vLLM).
    """
    print(f"B·∫Øt ƒë·∫ßu t·∫£i m√¥ h√¨nh: {config.LLM_MODEL_NAME} (ch·∫ø ƒë·ªô Transformers)")
    
    # S·ª≠ d·ª•ng quantization n·∫øu c√≥ GPU
    quantization_config = None
    if torch.cuda.is_available():
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
        print("üöÄ S·ª≠ d·ª•ng 4-bit quantization cho GPU")
    else:
        print("üíª Ch·∫°y tr√™n CPU (kh√¥ng quantization)")

    tokenizer = AutoTokenizer.from_pretrained(
        config.LLM_MODEL_NAME,
        cache_dir=config.MODEL_CACHE_DIR,
    )

    model = AutoModelForCausalLM.from_pretrained(
        config.LLM_MODEL_NAME,
        quantization_config=quantization_config,
        device_map="auto" if torch.cuda.is_available() else None,
        cache_dir=config.MODEL_CACHE_DIR,
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32
    )
    
    print("‚úÖ T·∫£i m√¥ h√¨nh LLM th√†nh c√¥ng (Transformers Pipeline).")

    text_generator = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        do_sample=True,
        temperature=0.1,
        return_full_text=False
    )
    
    return HuggingFacePipeline(pipeline=text_generator)

def load_embedding_model():
    """T·∫£i m√¥ h√¨nh embedding."""
    print(f"B·∫Øt ƒë·∫ßu t·∫£i embedding: {config.EMBEDDING_MODEL_NAME}")
    embeddings = HuggingFaceEmbeddings(
        model_name=config.EMBEDDING_MODEL_NAME,
        model_kwargs={'device': 'cuda'},
        cache_folder=config.MODEL_CACHE_DIR
    )
    print("‚úÖ M√¥ h√¨nh Embedding ƒë√£ s·∫µn s√†ng.")
    return embeddings

# services.py

# ... (H√†m load_llm_pipeline v√† load_embedding_model gi·ªØ nguy√™n) ...

def create_database_connection():
    """
    T·∫°o k·∫øt n·ªëi PostgreSQL database v√† SQL Database cho llama-index.
    """
    try:
        # S·ª≠ d·ª•ng DATABASE_URL t·ª´ .env file
        database_url = config.DATABASE_URL
        if not database_url:
            print("‚ùå Kh√¥ng t√¨m th·∫•y DATABASE_URL trong file .env")
            return None, None
        
        # T·∫°o SQLAlchemy engine t·ª´ DATABASE_URL
        engine = create_engine(database_url)
        
        # Test connection
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        
        print("‚úÖ K·∫øt n·ªëi database th√†nh c√¥ng!")
        return None, engine
        
    except Exception as e:
        print(f"‚ùå L·ªói k·∫øt n·ªëi database: {e}")
        return None, None

def query_database_direct(engine, query_text):
    """
    Th·ª±c thi truy v·∫•n SQL tr·ª±c ti·∫øp v√† tr·∫£ v·ªÅ k·∫øt qu·∫£.
    """
    try:
        with engine.connect() as conn:
            result = conn.execute(text(query_text))
            rows = result.fetchall()
            
            # Chuy·ªÉn ƒë·ªïi th√†nh list of dict
            data = []
            for row in rows:
                # Convert Row to dict
                row_dict = row._asdict() if hasattr(row, '_asdict') else dict(row._mapping)
                data.append(row_dict)
            
            return data
    except Exception as e:
        print(f"‚ùå L·ªói th·ª±c thi query: {e}")
        return []

def get_product_info_from_db(engine, search_term):
    """
    T√¨m ki·∫øm th√¥ng tin s·∫£n ph·∫©m t·ª´ database d·ª±a tr√™n t·ª´ kh√≥a.
    """
    query = """
    SELECT 
        p.id,
        p.name,
        p.description,
        p.price,
        p.sale_price,
        c.name as category_name,
        pv.size,
        pv.color,
        pv.stock,
        pi.image_url
    FROM products p
    LEFT JOIN categories c ON p.category_id = c.id
    LEFT JOIN product_variants pv ON p.id = pv.product_id
    LEFT JOIN product_images pi ON p.id = pi.product_id AND pi.is_primary = true
    WHERE LOWER(p.name) LIKE LOWER(:search1) 
       OR LOWER(p.description) LIKE LOWER(:search2)
       OR LOWER(c.name) LIKE LOWER(:search3)
    ORDER BY p.id, pv.id
    LIMIT 10
    """
    
    search_pattern = f"%{search_term}%"
    try:
        with engine.connect() as conn:
            result = conn.execute(text(query), {
                'search1': search_pattern,
                'search2': search_pattern, 
                'search3': search_pattern
            })
            rows = result.fetchall()
            
            # X·ª≠ l√Ω k·∫øt qu·∫£
            products = {}
            for row in rows:
                # Convert Row to dict for easier access
                row_dict = row._asdict() if hasattr(row, '_asdict') else dict(row._mapping)
                
                product_id = row_dict['id']
                if product_id not in products:
                    products[product_id] = {
                        'id': row_dict['id'],
                        'name': row_dict['name'],
                        'description': row_dict['description'],
                        'price': row_dict['price'],
                        'sale_price': row_dict['sale_price'],
                        'category': row_dict['category_name'],
                        'image': row_dict['image_url'],
                        'variants': []
                    }
                
                if row_dict['size'] and row_dict['color']:  # size and color exist
                    products[product_id]['variants'].append({
                        'size': row_dict['size'],
                        'color': row_dict['color'],
                        'stock': row_dict['stock']
                    })
            
            return list(products.values())
            
    except Exception as e:
        print(f"‚ùå L·ªói t√¨m ki·∫øm s·∫£n ph·∫©m: {e}")
        return []

def get_order_info_from_db(engine, search_term):
    """
    T√¨m ki·∫øm th√¥ng tin ƒë∆°n h√†ng t·ª´ database.
    """
    query = """
    SELECT 
        o.id,
        o.order_number,
        o.full_name,
        o.phone,
        o.email,
        o.status,
        o.created_at,
        oi.product_name,
        oi.quantity,
        oi.price,
        oi.subtotal
    FROM orders o
    LEFT JOIN order_items oi ON o.id = oi.order_id
    WHERE LOWER(o.order_number) LIKE LOWER(:search1)
       OR LOWER(o.full_name) LIKE LOWER(:search2)
       OR LOWER(o.phone) LIKE LOWER(:search3)
       OR LOWER(o.email) LIKE LOWER(:search4)
    ORDER BY o.created_at DESC
    LIMIT 20
    """
    
    search_pattern = f"%{search_term}%"
    try:
        with engine.connect() as conn:
            result = conn.execute(text(query), {
                'search1': search_pattern,
                'search2': search_pattern,
                'search3': search_pattern,
                'search4': search_pattern
            })
            rows = result.fetchall()
            
            # X·ª≠ l√Ω k·∫øt qu·∫£
            orders = {}
            for row in rows:
                # Convert Row to dict for easier access
                row_dict = row._asdict() if hasattr(row, '_asdict') else dict(row._mapping)
                
                order_id = row_dict['id']
                if order_id not in orders:
                    orders[order_id] = {
                        'id': row_dict['id'],
                        'order_number': row_dict['order_number'],
                        'customer_name': row_dict['full_name'],
                        'phone': row_dict['phone'],
                        'email': row_dict['email'],
                        'status': row_dict['status'],
                        'created_at': row_dict['created_at'],
                        'items': []
                    }
                
                if row_dict['product_name']:  # product_name exists
                    orders[order_id]['items'].append({
                        'product_name': row_dict['product_name'],
                        'quantity': row_dict['quantity'],
                        'price': row_dict['price'],
                        'subtotal': row_dict['subtotal']
                    })
            
            return list(orders.values())
            
    except Exception as e:
        print(f"‚ùå L·ªói t√¨m ki·∫øm ƒë∆°n h√†ng: {e}")
        return []

def create_rag_chain(llm, embeddings):
    """
    T·ª± ƒë·ªông QU√âT th∆∞ m·ª•c DATA_DIR, n·∫°p T·∫§T C·∫¢ c√°c file (.csv, .pdf, .txt)
    v√† x√¢y d·ª±ng RAG chain v·ªõi t√≠ch h·ª£p database.
    """
    print(f"B·∫Øt ƒë·∫ßu qu√©t th∆∞ m·ª•c ki·∫øn th·ª©c: {config.DATA_DIR}")
    
    all_documents = [] # List ƒë·ªÉ ch·ª©a t·∫•t c·∫£ t√†i li·ªáu

    # --- 1. QU√âT TH∆Ø M·ª§C V√Ä LOAD FILE ---
    try:
        # L·∫•y danh s√°ch file trong th∆∞ m·ª•c DATA_DIR
        filenames = os.listdir(config.DATA_DIR)
        
        for filename in filenames:
            filepath = os.path.join(config.DATA_DIR, filename)
            
            # --- X·ª≠ l√Ω file CSV (Logic c≈© c·ªßa b·∫°n) ---
            if filename.endswith(".csv"):
                print(f"  [CSV] ƒêang x·ª≠ l√Ω file: {filename}")
                df = pd.read_csv(filepath)
                for _, row in df.iterrows():
                    content = f"T√™n: {row['product_name']}\n"
                    content += f"Lo·∫°i: {row['category']}\n"
                    if row['price'] > 0:
                        content += f"Gi√°: {row['price']:,} VNƒê\n"
                    content += f"M√¥ t·∫£: {row['description']}"
                    doc = Document(page_content=content, metadata={"source": filename})
                    all_documents.append(doc)

            # --- X·ª≠ l√Ω file Text ---
            elif filename.endswith(".txt"):
                print(f"  [TXT] ƒêang x·ª≠ l√Ω file: {filename}")
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                doc = Document(page_content=content, metadata={"source": filename})
                all_documents.append(doc)

            # --- B·ªè qua PDF t·∫°m th·ªùi ƒë·ªÉ tr√°nh l·ªói dependency ---
            elif filename.endswith(".pdf"):
                print(f"  [PDF] B·ªè qua file PDF: {filename} (ch∆∞a h·ªó tr·ª£)")
            
            else:
                print(f"  [SKIP] B·ªè qua file kh√¥ng h·ªó tr·ª£: {filename}")

    except FileNotFoundError:
        print(f"‚ö†Ô∏è L·ªñI: Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c {config.DATA_DIR}.")
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªñI khi qu√©t th∆∞ m·ª•c: {e}")

    # --- 2. KI·ªÇM TRA D·ªÆ LI·ªÜU ---
    if not all_documents:
        print("‚ö†Ô∏è C·∫¢NH B√ÅO: Kh√¥ng n·∫°p ƒë∆∞·ª£c b·∫•t k·ª≥ t√†i li·ªáu n√†o. Bot s·∫Ω kh√¥ng c√≥ ki·∫øn th·ª©c.")
        # T·∫°o m·ªôt t√†i li·ªáu r·ªóng ƒë·ªÉ tr√°nh l·ªói
        all_documents = [Document(page_content="Kh√¥ng c√≥ ki·∫øn th·ª©c.")]

    print(all_documents[0:2]) # In 2 t√†i li·ªáu ƒë·∫ßu ƒë·ªÉ ki·ªÉm tra

    # --- 3. T·∫†O VECTOR STORE (Nh∆∞ c≈©) ---
    print("Kh·ªüi t·∫°o Vector Store FAISS...")
    vector_store = FAISS.from_documents(all_documents, embeddings)
    retriever = vector_store.as_retriever(search_kwargs={"k": 3}) # L·∫•y 3 k·∫øt qu·∫£
    print("‚úÖ Vector Store FAISS v√† Retriever ƒë√£ s·∫µn s√†ng.")

    # --- 4. T·∫†O DATABASE CONNECTION ---
    sql_database, engine = create_database_connection()

    # --- 5. T·∫†O HYBRID RETRIEVER ---
    def hybrid_retriever(question):
        """
        K·∫øt h·ª£p t√¨m ki·∫øm vector v√† database query.
        """
        # 1. T√¨m ki·∫øm t·ª´ vector store
        vector_results = retriever.invoke(question)
        
        # 2. T√¨m ki·∫øm t·ª´ database n·∫øu c√≥ k·∫øt n·ªëi
        db_results = []
        if engine:
            # Ph√°t hi·ªán lo·∫°i c√¢u h·ªèi v√† t√¨m ki·∫øm ph√π h·ª£p
            question_lower = question.lower()
            
            # C√¢u h·ªèi v·ªÅ s·∫£n ph·∫©m
            if any(keyword in question_lower for keyword in ['s·∫£n ph·∫©m', '√°o', 'qu·∫ßn', 'gi√°', 'mua', 'b√°n']):
                products = get_product_info_from_db(engine, question)
                for product in products:
                    content = f"S·∫£n ph·∫©m: {product['name']}\n"
                    content += f"Danh m·ª•c: {product['category']}\n"
                    content += f"Gi√°: {product['price']:,.0f} VNƒê\n"
                    if product['sale_price']:
                        content += f"Gi√° khuy·∫øn m√£i: {product['sale_price']:,.0f} VNƒê\n"
                    content += f"M√¥ t·∫£: {product['description']}\n"
                    if product['variants']:
                        content += "Bi·∫øn th·ªÉ:\n"
                        for variant in product['variants']:
                            content += f"  - Size {variant['size']}, M√†u {variant['color']}, T·ªìn kho: {variant['stock']}\n"
                    
                    db_results.append(Document(page_content=content, metadata={"source": "database_products"}))
            
            # C√¢u h·ªèi v·ªÅ ƒë∆°n h√†ng
            elif any(keyword in question_lower for keyword in ['ƒë∆°n h√†ng', 'order', 'mua', 'kh√°ch h√†ng']):
                orders = get_order_info_from_db(engine, question)
                for order in orders:
                    content = f"ƒê∆°n h√†ng: {order['order_number']}\n"
                    content += f"Kh√°ch h√†ng: {order['customer_name']}\n"
                    content += f"ƒêi·ªán tho·∫°i: {order['phone']}\n"
                    content += f"Tr·∫°ng th√°i: {order['status']}\n"
                    content += f"Ng√†y t·∫°o: {order['created_at']}\n"
                    if order['items']:
                        content += "S·∫£n ph·∫©m:\n"
                        for item in order['items']:
                            content += f"  - {item['product_name']}: {item['quantity']} x {item['price']:,.0f} = {item['subtotal']:,.0f} VNƒê\n"
                    
                    db_results.append(Document(page_content=content, metadata={"source": "database_orders"}))
        
        # 3. K·∫øt h·ª£p k·∫øt qu·∫£
        all_results = vector_results + db_results
        return all_results[:5]  # Gi·ªõi h·∫°n 5 k·∫øt qu·∫£

    # --- 6. T·∫†O PROMPT V√Ä CHAIN (C·∫£i ti·∫øn) ---
    rag_template = """<s>[INST] B·∫°n l√† m·ªôt tr·ª£ l√Ω AI c·ªßa shop th·ªùi trang, chuy√™n nghi·ªáp v√† ch·ªâ tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.
B·∫°n ph·∫£i tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a v√†o "N·ªôi dung" ƒë∆∞·ª£c cung c·∫•p t·ª´ c∆° s·ªü d·ªØ li·ªáu v√† t√†i li·ªáu.

H∆∞·ªõng d·∫´n:
- ∆Øu ti√™n th√¥ng tin t·ª´ c∆° s·ªü d·ªØ li·ªáu (database_products, database_orders) v√¨ ƒë√¢y l√† d·ªØ li·ªáu th·ª±c t·∫ø nh·∫•t
- N·∫øu kh√¥ng c√≥ th√¥ng tin ƒë·∫ßy ƒë·ªß, h√£y n√≥i: "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ƒë·∫ßy ƒë·ªß v·ªÅ y√™u c·∫ßu n√†y."
- ƒê·ªëi v·ªõi s·∫£n ph·∫©m: cung c·∫•p t√™n, gi√°, danh m·ª•c, bi·∫øn th·ªÉ (size, m√†u, t·ªìn kho)
- ƒê·ªëi v·ªõi ƒë∆°n h√†ng: cung c·∫•p m√£ ƒë∆°n, tr·∫°ng th√°i, th√¥ng tin kh√°ch h√†ng
- Tr·∫£ l·ªùi th√¢n thi·ªán v√† h·ªØu √≠ch

N·ªôi dung:
{context}

C√¢u h·ªèi: {question} [/INST]
"""
    rag_prompt = PromptTemplate.from_template(rag_template)

    def format_docs(docs):
        return "\n\n---\n\n".join(doc.page_content for doc in docs)

    # T·∫°o chain v·ªõi c√∫ ph√°p t∆∞∆°ng th√≠ch
    def enhanced_context_retriever(inputs):
        """Retrieve v√† format context t·ª´ hybrid retriever."""
        question = inputs if isinstance(inputs, str) else inputs.get("question", "")
        docs = hybrid_retriever(question)
        return format_docs(docs)

    # RunnableLambda already imported at top
    
    rag_chain = (
        RunnableLambda(lambda x: {"context": enhanced_context_retriever(x), "question": x})
        | rag_prompt
        | llm
        | StrOutputParser()
    )
    print("‚úÖ Pipeline RAG v·ªõi database integration ho√†n ch·ªânh ƒë√£ s·∫µn s√†ng.")
    return rag_chain