import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
from langchain_huggingface import HuggingFacePipeline
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from huggingface_hub import login
# from langchain_community.llms import VLLM  # B·ªè vLLM
import pandas as pd
from langchain_community.document_loaders import PyPDFLoader, TextLoader

# Import c·∫•u h√¨nh t·ª´ file config.py
import config

def login_huggingface():
    """ƒêƒÉng nh·∫≠p v√†o Hugging Face."""
    if config.HUGGINGFACE_TOKEN:
        login(token=config.HUGGINGFACE_TOKEN)
        print("‚úÖ ƒê√£ ƒëƒÉng nh·∫≠p Hugging Face!")
    else:
        print("‚ö†Ô∏è C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y HUGGINGFACE_ACCESS_TOKEN.")

# def load_llm_pipeline():
#     """
#     T·∫£i m√¥ h√¨nh LLM (4-bit) v√† t·∫°o ra HuggingFacePipeline c·ªßa LangChain.
#     """
#     print(f"B·∫Øt ƒë·∫ßu t·∫£i m√¥ h√¨nh: {config.LLM_MODEL_NAME} (ch·∫ø ƒë·ªô 4-bit)")
    
#     quantization_config = BitsAndBytesConfig(
#         load_in_4bit=True,
#         bnb_4bit_compute_dtype=torch.bfloat16,
#     )

#     tokenizer = AutoTokenizer.from_pretrained(
#         config.LLM_MODEL_NAME,
#         cache_dir=config.MODEL_CACHE_DIR,
#     )

#     model = AutoModelForCausalLM.from_pretrained(
#         config.LLM_MODEL_NAME,
#         quantization_config=quantization_config,
#         device_map="auto",
#         cache_dir=config.MODEL_CACHE_DIR
#     )
    
#     print("‚úÖ T·∫£i m√¥ h√¨nh LLM th√†nh c√¥ng (ch·∫ø ƒë·ªô 4-bit).")

#     text_generator = pipeline(
#         "text-generation",
#         model=model,
#         tokenizer=tokenizer,
#         max_new_tokens=512,
#         do_sample=True,
#         temperature=0.1,
#         return_full_text=False
#     )
    
#     return HuggingFacePipeline(pipeline=text_generator)

def load_llm_pipeline():
    """
    T·∫£i m√¥ h√¨nh LLM b·∫±ng Transformers Pipeline (kh√¥ng c·∫ßn vLLM).
    """
    print(f"B·∫Øt ƒë·∫ßu t·∫£i m√¥ h√¨nh: {config.LLM_MODEL_NAME} (ch·∫ø ƒë·ªô Transformers)")
    
    # S·ª≠ d·ª•ng quantization n·∫øu c√≥ GPU
    quantization_config = None
    if torch.cuda.is_available():
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
        print("üöÄ S·ª≠ d·ª•ng 4-bit quantization cho GPU")
    else:
        print("üíª Ch·∫°y tr√™n CPU (kh√¥ng quantization)")

    tokenizer = AutoTokenizer.from_pretrained(
        config.LLM_MODEL_NAME,
        cache_dir=config.MODEL_CACHE_DIR,
    )

    model = AutoModelForCausalLM.from_pretrained(
        config.LLM_MODEL_NAME,
        quantization_config=quantization_config,
        device_map="auto" if torch.cuda.is_available() else None,
        cache_dir=config.MODEL_CACHE_DIR,
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32
    )
    
    print("‚úÖ T·∫£i m√¥ h√¨nh LLM th√†nh c√¥ng (Transformers Pipeline).")

    text_generator = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        do_sample=True,
        temperature=0.1,
        return_full_text=False
    )
    
    return HuggingFacePipeline(pipeline=text_generator)

def load_embedding_model():
    """T·∫£i m√¥ h√¨nh embedding."""
    print(f"B·∫Øt ƒë·∫ßu t·∫£i embedding: {config.EMBEDDING_MODEL_NAME}")
    embeddings = HuggingFaceEmbeddings(
        model_name=config.EMBEDDING_MODEL_NAME,
        model_kwargs={'device': 'cuda'},
        cache_folder=config.MODEL_CACHE_DIR
    )
    print("‚úÖ M√¥ h√¨nh Embedding ƒë√£ s·∫µn s√†ng.")
    return embeddings

# services.py

# ... (H√†m load_llm_pipeline v√† load_embedding_model gi·ªØ nguy√™n) ...

def create_rag_chain(llm, embeddings):
    """
    T·ª± ƒë·ªông QU√âT th∆∞ m·ª•c DATA_DIR, n·∫°p T·∫§T C·∫¢ c√°c file (.csv, .pdf, .txt)
    v√† x√¢y d·ª±ng RAG chain.
    """
    print(f"B·∫Øt ƒë·∫ßu qu√©t th∆∞ m·ª•c ki·∫øn th·ª©c: {config.DATA_DIR}")
    
    all_documents = [] # List ƒë·ªÉ ch·ª©a t·∫•t c·∫£ t√†i li·ªáu

    # --- 1. QU√âT TH∆Ø M·ª§C V√Ä LOAD FILE ---
    try:
        # L·∫•y danh s√°ch file trong th∆∞ m·ª•c DATA_DIR
        filenames = os.listdir(config.DATA_DIR)
        
        for filename in filenames:
            filepath = os.path.join(config.DATA_DIR, filename)
            
            # --- X·ª≠ l√Ω file CSV (Logic c≈© c·ªßa b·∫°n) ---
            if filename.endswith(".csv"):
                print(f"  [CSV] ƒêang x·ª≠ l√Ω file: {filename}")
                df = pd.read_csv(filepath)
                for _, row in df.iterrows():
                    content = f"T√™n: {row['product_name']}\n"
                    content += f"Lo·∫°i: {row['category']}\n"
                    if row['price'] > 0:
                        content += f"Gi√°: {row['price']:,} VNƒê\n"
                    content += f"M√¥ t·∫£: {row['description']}"
                    doc = Document(page_content=content, metadata={"source": filename})
                    all_documents.append(doc)

            # --- X·ª≠ l√Ω file PDF ---
            elif filename.endswith(".pdf"):
                print(f"  [PDF] ƒêang x·ª≠ l√Ω file: {filename}")
                loader = PyPDFLoader(filepath)
                docs = loader.load() # T·∫£i v√† t√°ch trang
                all_documents.extend(docs) # Th√™m c√°c trang v√†o list

            # --- X·ª≠ l√Ω file Text ---
            elif filename.endswith(".txt"):
                print(f"  [TXT] ƒêang x·ª≠ l√Ω file: {filename}")
                loader = TextLoader(filepath, encoding="utf-8")
                docs = loader.load()
                all_documents.extend(docs)
            
            else:
                print(f"  [SKIP] B·ªè qua file kh√¥ng h·ªó tr·ª£: {filename}")

    except FileNotFoundError:
        print(f"‚ö†Ô∏è L·ªñI: Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c {config.DATA_DIR}.")
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªñI khi qu√©t th∆∞ m·ª•c: {e}")

    # --- 2. KI·ªÇM TRA D·ªÆ LI·ªÜU ---
    if not all_documents:
        print("‚ö†Ô∏è C·∫¢NH B√ÅO: Kh√¥ng n·∫°p ƒë∆∞·ª£c b·∫•t k·ª≥ t√†i li·ªáu n√†o. Bot s·∫Ω kh√¥ng c√≥ ki·∫øn th·ª©c.")
        # T·∫°o m·ªôt t√†i li·ªáu r·ªóng ƒë·ªÉ tr√°nh l·ªói
        all_documents = [Document(page_content="Kh√¥ng c√≥ ki·∫øn th·ª©c.")]

    print(all_documents[0:2]) # In 2 t√†i li·ªáu ƒë·∫ßu ƒë·ªÉ ki·ªÉm tra

    # --- 3. T·∫†O VECTOR STORE (Nh∆∞ c≈©) ---
    print("Kh·ªüi t·∫°o Vector Store FAISS...")
    vector_store = FAISS.from_documents(all_documents, embeddings)
    retriever = vector_store.as_retriever(search_kwargs={"k": 3}) # L·∫•y 3 k·∫øt qu·∫£
    print("‚úÖ Vector Store FAISS v√† Retriever ƒë√£ s·∫µn s√†ng.")

    # --- 4. T·∫†O PROMPT V√Ä CHAIN (Nh∆∞ c≈©) ---
    rag_template = """<s>[INST] B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch, chuy√™n nghi·ªáp v√† ch·ªâ tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.
B·∫°n ph·∫£i tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng D·ª∞A HO√ÄN TO√ÄN v√†o "N·ªôi dung" ƒë∆∞·ª£c cung c·∫•p.
N·∫øu "N·ªôi dung" kh√¥ng ch·ª©a th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√≥i: "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu."
Kh√¥ng ƒë∆∞·ª£c b·ªãa ƒë·∫∑t th√¥ng tin.

N·ªôi dung:
{context}

C√¢u h·ªèi: {question} [/INST]
"""
    rag_prompt = PromptTemplate.from_template(rag_template)

    def format_docs(docs):
        return "\n\n---\n\n".join(doc.page_content for doc in docs)

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | rag_prompt
        | llm
        | StrOutputParser()
    )
    print("‚úÖ Pipeline RAG (t·ª± ƒë·ªông qu√©t) ho√†n ch·ªânh ƒë√£ s·∫µn s√†ng.")
    return rag_chain