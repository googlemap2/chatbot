import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
from langchain_huggingface import HuggingFacePipeline
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from huggingface_hub import login
import pandas as pd
import psycopg2
from psycopg2.extras import RealDictCursor
from sqlalchemy import create_engine, text
import json
import re
# Import c·∫•u h√¨nh t·ª´ file config.py
import config

def login_huggingface():
    """ƒêƒÉng nh·∫≠p v√†o Hugging Face."""
    if config.HUGGINGFACE_TOKEN:
        login(token=config.HUGGINGFACE_TOKEN)
        print("‚úÖ ƒê√£ ƒëƒÉng nh·∫≠p Hugging Face!")
    else:
        print("‚ö†Ô∏è C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y HUGGINGFACE_ACCESS_TOKEN.")

# def load_llm_pipeline():
#     """
#     T·∫£i m√¥ h√¨nh LLM (4-bit) v√† t·∫°o ra HuggingFacePipeline c·ªßa LangChain.
#     """
#     print(f"B·∫Øt ƒë·∫ßu t·∫£i m√¥ h√¨nh: {config.LLM_MODEL_NAME} (ch·∫ø ƒë·ªô 4-bit)")
    
#     quantization_config = BitsAndBytesConfig(
#         load_in_4bit=True,
#         bnb_4bit_compute_dtype=torch.bfloat16,
#     )

#     tokenizer = AutoTokenizer.from_pretrained(
#         config.LLM_MODEL_NAME,
#         cache_dir=config.MODEL_CACHE_DIR,
#     )

#     model = AutoModelForCausalLM.from_pretrained(
#         config.LLM_MODEL_NAME,
#         quantization_config=quantization_config,
#         device_map="auto",
#         cache_dir=config.MODEL_CACHE_DIR
#     )
    
#     print("‚úÖ T·∫£i m√¥ h√¨nh LLM th√†nh c√¥ng (ch·∫ø ƒë·ªô 4-bit).")

#     text_generator = pipeline(
#         "text-generation",
#         model=model,
#         tokenizer=tokenizer,
#         max_new_tokens=512,
#         do_sample=True,
#         temperature=0.1,
#         return_full_text=False
#     )
    
#     return HuggingFacePipeline(pipeline=text_generator)

def load_llm_pipeline():
    """
    T·∫£i m√¥ h√¨nh LLM b·∫±ng Transformers Pipeline (kh√¥ng c·∫ßn vLLM).
    """
    print(f"B·∫Øt ƒë·∫ßu t·∫£i m√¥ h√¨nh: {config.LLM_MODEL_NAME} (ch·∫ø ƒë·ªô Transformers)")
    
    # S·ª≠ d·ª•ng quantization n·∫øu c√≥ GPU
    quantization_config = None
    if torch.cuda.is_available():
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
        print("üöÄ S·ª≠ d·ª•ng 4-bit quantization cho GPU")
    else:
        print("üíª Ch·∫°y tr√™n CPU (kh√¥ng quantization)")

    tokenizer = AutoTokenizer.from_pretrained(
        config.LLM_MODEL_NAME,
        cache_dir=config.MODEL_CACHE_DIR,
    )

    model = AutoModelForCausalLM.from_pretrained(
        config.LLM_MODEL_NAME,
        quantization_config=quantization_config,
        device_map="auto" if torch.cuda.is_available() else None,
        cache_dir=config.MODEL_CACHE_DIR,
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32
    )
    
    print("‚úÖ T·∫£i m√¥ h√¨nh LLM th√†nh c√¥ng (Transformers Pipeline).")

    # Add pad token if missing
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    text_generator = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=256,  # Gi·∫£m t·ª´ 512 xu·ªëng 256
        do_sample=True,
        temperature=0.7,  # TƒÉng ƒë·ªÉ nhanh h∆°n
        top_p=0.9,
        return_full_text=False,
        pad_token_id=tokenizer.eos_token_id
    )
    
    return HuggingFacePipeline(pipeline=text_generator)

def load_embedding_model():
    """T·∫£i m√¥ h√¨nh embedding."""
    print(f"B·∫Øt ƒë·∫ßu t·∫£i embedding: {config.EMBEDDING_MODEL_NAME}")
    embeddings = HuggingFaceEmbeddings(
        model_name=config.EMBEDDING_MODEL_NAME,
        model_kwargs={'device': 'cuda'},
        cache_folder=config.MODEL_CACHE_DIR
    )
    print("‚úÖ M√¥ h√¨nh Embedding ƒë√£ s·∫µn s√†ng.")
    return embeddings

# services.py

# ... (H√†m load_llm_pipeline v√† load_embedding_model gi·ªØ nguy√™n) ...

def create_database_connection():
    """
    T·∫°o k·∫øt n·ªëi PostgreSQL database v√† SQL Database cho llama-index.
    """
    try:
        # S·ª≠ d·ª•ng DATABASE_URL t·ª´ .env file
        database_url = config.DATABASE_URL
        if not database_url:
            print("‚ùå Kh√¥ng t√¨m th·∫•y DATABASE_URL trong file .env")
            return None, None
        
        # T·∫°o SQLAlchemy engine t·ª´ DATABASE_URL v·ªõi connection pooling
        engine = create_engine(
            database_url,
            pool_size=5,
            max_overflow=10,
            pool_pre_ping=True
        )
        
        # Test connection
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        
        print("‚úÖ K·∫øt n·ªëi database th√†nh c√¥ng!")
        return None, engine
        
    except Exception as e:
        print(f"‚ùå L·ªói k·∫øt n·ªëi database: {e}")
        return None, None

def query_database_direct(engine, query_text):
    """
    Th·ª±c thi truy v·∫•n SQL tr·ª±c ti·∫øp v√† tr·∫£ v·ªÅ k·∫øt qu·∫£.
    """
    try:
        with engine.connect() as conn:
            result = conn.execute(text(query_text))
            rows = result.fetchall()
            
            # Chuy·ªÉn ƒë·ªïi th√†nh list of dict
            data = []
            for row in rows:
                # Convert Row to dict
                row_dict = row._asdict() if hasattr(row, '_asdict') else dict(row._mapping)
                data.append(row_dict)
            
            return data
    except Exception as e:
        print(f"‚ùå L·ªói th·ª±c thi query: {e}")
        return []

def get_product_info_from_db(engine, search_term):
    """
    T√¨m ki·∫øm th√¥ng tin s·∫£n ph·∫©m t·ª´ database d·ª±a tr√™n t·ª´ kh√≥a.
    """
    query = """
    SELECT 
        p.id,
        p.name,
        p.description,
        p.price,
        p.sale_price,
        p.stock,
        c.name as category_name,
        pv.sku,
        pv.size,
        pv.color,
        pv.stock as variant_stock
    FROM products p
    LEFT JOIN categories c ON p.category_id = c.id
    LEFT JOIN product_variants pv ON p.id = pv.product_id
    WHERE LOWER(p.name) LIKE LOWER(:search1) 
       OR LOWER(p.description) LIKE LOWER(:search2)
       OR LOWER(c.name) LIKE LOWER(:search3)
       OR LOWER(pv.sku) LIKE LOWER(:search4)
    ORDER BY p.id, pv.id
    LIMIT 10
    """
    
    search_pattern = f"%{search_term}%"
    try:
        with engine.connect() as conn:
            result = conn.execute(text(query), {
                'search1': search_pattern,
                'search2': search_pattern, 
                'search3': search_pattern,
                'search4': search_pattern
            })
            rows = result.fetchall()
            
            # X·ª≠ l√Ω k·∫øt qu·∫£ v·ªõi group theo product_id
            products = {}
            for row in rows:
                # Convert Row to dict for easier access
                row_dict = row._asdict() if hasattr(row, '_asdict') else dict(row._mapping)
                
                product_id = row_dict['id']
                if product_id not in products:
                    products[product_id] = {
                        'id': row_dict['id'],
                        'name': row_dict['name'],
                        'description': row_dict['description'],
                        'price': row_dict['price'],
                        'sale_price': row_dict['sale_price'],
                        'stock': row_dict['stock'],
                        'category': row_dict['category_name'],
                        'variants': []
                    }
                
                # Th√™m variant n·∫øu c√≥
                if row_dict['sku']:
                    products[product_id]['variants'].append({
                        'sku': row_dict['sku'],
                        'size': row_dict['size'],
                        'color': row_dict['color'],
                        'stock': row_dict['variant_stock']
                    })
            
            return list(products.values())
            
            return products
            
    except Exception as e:
        print(f"‚ùå L·ªói t√¨m ki·∫øm s·∫£n ph·∫©m: {e}")
        return []

def get_order_info_from_db(engine, search_term):
    """
    T√¨m ki·∫øm th√¥ng tin ƒë∆°n h√†ng t·ª´ database.
    """
    # Map tr·∫°ng th√°i t·ª´ database sang ti·∫øng Vi·ªát
    STATUS_MAP = {
        'pending': 'Ch·ªù x√°c nh·∫≠n',
        'confirmed': 'ƒê√£ x√°c nh·∫≠n',
        'shipping': 'ƒêang giao h√†ng',
        'delivered': 'ƒê√£ giao h√†ng',
        'cancelled': 'ƒê√£ h·ªßy'
    }
    
    query = """
    SELECT 
        o.id,
        o.order_number,
        o.full_name,
        o.phone,
        o.email,
        o.status,
        o.created_at,
        oi.product_name,
        oi.quantity,
        oi.price,
        oi.subtotal
    FROM orders o
    LEFT JOIN order_items oi ON o.id = oi.order_id
    WHERE LOWER(o.order_number) LIKE LOWER(:search1)
       OR LOWER(o.full_name) LIKE LOWER(:search2)
       OR LOWER(o.phone) LIKE LOWER(:search3)
       OR LOWER(o.email) LIKE LOWER(:search4)
    ORDER BY o.created_at DESC
    LIMIT 5
    """
    
    search_pattern = f"%{search_term}%"
    try:
        with engine.connect() as conn:
            result = conn.execute(text(query), {
                'search1': search_pattern,
                'search2': search_pattern,
                'search3': search_pattern,
                'search4': search_pattern
            })
            rows = result.fetchall()
            
            # X·ª≠ l√Ω k·∫øt qu·∫£
            orders = {}
            for row in rows:
                # Convert Row to dict for easier access
                row_dict = row._asdict() if hasattr(row, '_asdict') else dict(row._mapping)
                
                order_id = row_dict['id']
                if order_id not in orders:
                    # Map tr·∫°ng th√°i sang ti·∫øng Vi·ªát
                    status_vi = STATUS_MAP.get(row_dict['status'].lower(), row_dict['status'])
                    
                    orders[order_id] = {
                        'id': row_dict['id'],
                        'order_number': row_dict['order_number'],
                        'customer_name': row_dict['full_name'],
                        'phone': row_dict['phone'],
                        'email': row_dict['email'],
                        'status': status_vi,
                        'created_at': row_dict['created_at'],
                        'items': []
                    }
                
                if row_dict['product_name']:  # product_name exists
                    orders[order_id]['items'].append({
                        'product_name': row_dict['product_name'],
                        'quantity': row_dict['quantity'],
                        'price': row_dict['price'],
                        'subtotal': row_dict['subtotal']
                    })
            
            return list(orders.values())
            
    except Exception as e:
        print(f"‚ùå L·ªói t√¨m ki·∫øm ƒë∆°n h√†ng: {e}")
        return []

def create_rag_chain(llm, embeddings):
    """
    T·ª± ƒë·ªông QU√âT th∆∞ m·ª•c DATA_DIR, n·∫°p T·∫§T C·∫¢ c√°c file (.csv, .pdf, .txt)
    v√† x√¢y d·ª±ng RAG chain v·ªõi t√≠ch h·ª£p database.
    """
    print(f"B·∫Øt ƒë·∫ßu qu√©t th∆∞ m·ª•c ki·∫øn th·ª©c: {config.DATA_DIR}")
    
    all_documents = [] # List ƒë·ªÉ ch·ª©a t·∫•t c·∫£ t√†i li·ªáu

    # --- 1. QU√âT TH∆Ø M·ª§C V√Ä LOAD FILE ---
    try:
        # L·∫•y danh s√°ch file trong th∆∞ m·ª•c DATA_DIR
        filenames = os.listdir(config.DATA_DIR)
        
        for filename in filenames:
            filepath = os.path.join(config.DATA_DIR, filename)
            
            # --- X·ª≠ l√Ω file CSV (Logic c≈© c·ªßa b·∫°n) ---
            if filename.endswith(".csv"):
                print(f"  [CSV] ƒêang x·ª≠ l√Ω file: {filename}")
                df = pd.read_csv(filepath)
                for _, row in df.iterrows():
                    content = f"T√™n: {row['product_name']}\n"
                    content += f"Lo·∫°i: {row['category']}\n"
                    if row['price'] > 0:
                        content += f"Gi√°: {row['price']:,} VNƒê\n"
                    content += f"M√¥ t·∫£: {row['description']}"
                    doc = Document(page_content=content, metadata={"source": filename})
                    all_documents.append(doc)

            # --- X·ª≠ l√Ω file Text ---
            elif filename.endswith(".txt"):
                print(f"  [TXT] ƒêang x·ª≠ l√Ω file: {filename}")
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                doc = Document(page_content=content, metadata={"source": filename})
                all_documents.append(doc)

            # --- B·ªè qua PDF t·∫°m th·ªùi ƒë·ªÉ tr√°nh l·ªói dependency ---
            elif filename.endswith(".pdf"):
                print(f"  [PDF] B·ªè qua file PDF: {filename} (ch∆∞a h·ªó tr·ª£)")
            
            else:
                print(f"  [SKIP] B·ªè qua file kh√¥ng h·ªó tr·ª£: {filename}")

    except FileNotFoundError:
        print(f"‚ö†Ô∏è L·ªñI: Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c {config.DATA_DIR}.")
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªñI khi qu√©t th∆∞ m·ª•c: {e}")

    # --- 2. KI·ªÇM TRA D·ªÆ LI·ªÜU ---
    if not all_documents:
        print("‚ö†Ô∏è C·∫¢NH B√ÅO: Kh√¥ng n·∫°p ƒë∆∞·ª£c b·∫•t k·ª≥ t√†i li·ªáu n√†o. Bot s·∫Ω kh√¥ng c√≥ ki·∫øn th·ª©c.")
        # T·∫°o m·ªôt t√†i li·ªáu r·ªóng ƒë·ªÉ tr√°nh l·ªói
        all_documents = [Document(page_content="Kh√¥ng c√≥ ki·∫øn th·ª©c.")]

    print(all_documents[0:2]) # In 2 t√†i li·ªáu ƒë·∫ßu ƒë·ªÉ ki·ªÉm tra

    # --- 3. T·∫†O VECTOR STORE (Nh∆∞ c≈©) ---
    print("Kh·ªüi t·∫°o Vector Store FAISS...")
    vector_store = FAISS.from_documents(all_documents, embeddings)
    retriever = vector_store.as_retriever(search_kwargs={"k": 2}) # L·∫•y 2 k·∫øt qu·∫£
    print("‚úÖ Vector Store FAISS v√† Retriever ƒë√£ s·∫µn s√†ng.")

    # --- 4. T·∫†O DATABASE CONNECTION ---
    sql_database, engine = create_database_connection()

    # --- 5. T·∫†O HYBRID RETRIEVER ---
    def hybrid_retriever(question):
        """
        K·∫øt h·ª£p t√¨m ki·∫øm vector v√† database query.
        """
        question_lower = question.lower().strip()
        
        # Fast-path: X·ª≠ l√Ω c√¢u ch√†o - tr·∫£ v·ªÅ response c·ªë ƒë·ªãnh
        greetings = ['xin ch√†o', 'hello', 'hi', 'ch√†o', 'hey', 'ch√†o shop', 'alo']
        if any(greeting in question_lower for greeting in greetings) and len(question) < 30:
            return [Document(
                page_content="Kh√°ch h√†ng ch√†o h·ªèi. Tr·∫£ l·ªùi: 'D·∫°, ch√†o anh/ch·ªã! Shop em b√°n qu·∫ßn √°o th·ªùi trang, anh/Ch·ªã c·∫ßn em t∆∞ v·∫•n g√¨ ·∫°?'",
                metadata={"source": "greeting"}
            )]
        
        # Fast-path: X·ª≠ l√Ω c√¢u c·∫£m ∆°n
        thanks = ['c·∫£m ∆°n', 'thank', 'thanks', 'c√°m ∆°n', 'cam on']
        if any(thank in question_lower for thank in thanks):
            return [Document(
                page_content="Kh√°ch h√†ng c·∫£m ∆°n. Tr·∫£ l·ªùi: 'D·∫°, c·∫£m ∆°n anh/ch·ªã ƒë√£ gh√© thƒÉm c·ª≠a h√†ng, anh/Ch·ªã c√≥ c·∫ßn em t∆∞ v·∫•n th√™m g√¨ kh√¥ng ·∫°?'",
                metadata={"source": "thanks"}
            )]
        order_only_pattern = r'^ORD\d+$'
        if re.match(order_only_pattern, question.upper().strip()):
            # Chuy·ªÉn sang t√¨m ki·∫øm ƒë∆°n h√†ng - c·∫≠p nh·∫≠t c·∫£ question v√† question_lower
            question = 'ƒë∆°n h√†ng ' + question
            question_lower = question.lower()
            
        # 1. T√¨m ki·∫øm t·ª´ vector store
        vector_results = retriever.invoke(question)
        
        # 2. T√¨m ki·∫øm t·ª´ database n·∫øu c√≥ k·∫øt n·ªëi
        db_results = []
        if engine:
            # Ph√°t hi·ªán lo·∫°i c√¢u h·ªèi v√† t√¨m ki·∫øm ph√π h·ª£p
            question_lower = question.lower()
            
            # C√¢u h·ªèi v·ªÅ s·∫£n ph·∫©m
            if any(keyword in question_lower for keyword in ['s·∫£n ph·∫©m', '√°o', 'qu·∫ßn', 'gi√°', 'mua', 'b√°n', 't√¨m']):
                # Extract t√™n s·∫£n ph·∫©m ho·∫∑c m√£ SKU
                # Pattern 1: T√¨m SKU (c√≥ d·∫•u g·∫°ch ngang: ATN-PREMIUM-S-BLACK)
                sku_pattern = r'\b[A-Z0-9]+-[A-Z0-9-]+\b'
                sku_match = re.search(sku_pattern, question.upper())
                
                # Pattern 2: T√¨m t·ª´ kh√≥a sau "s·∫£n ph·∫©m", "t√¨m", "c√≥"
                keyword_pattern = r'(?:s·∫£n ph·∫©m|t√¨m|c√≥|mua|b√°n)\s+(.+?)(?:\s+kh√¥ng|\s+c√≥|\s*$)'
                keyword_match = re.search(keyword_pattern, question_lower, re.IGNORECASE)
                
                # ∆Øu ti√™n SKU, n·∫øu kh√¥ng c√≥ th√¨ d√πng keyword
                if sku_match:
                    search_term = sku_match.group(0)
                elif keyword_match:
                    search_term = keyword_match.group(1).strip()
                else:
                    search_term = question
                
                print(f"üîç DEBUG: T√¨m ki·∫øm s·∫£n ph·∫©m v·ªõi t·ª´ kh√≥a: '{search_term}'")
                products = get_product_info_from_db(engine, search_term)
                print(f"üîç DEBUG: T√¨m th·∫•y {len(products)} s·∫£n ph·∫©m")
                for product in products[:3]:  # Ch·ªâ l·∫•y 3 s·∫£n ph·∫©m ƒë·∫ßu
                    print(f"üîç DEBUG: S·∫£n ph·∫©m: {product['name']}, Gi√°: {product['price']}")
                    content = f"S·∫£n ph·∫©m: {product['name']}\n"
                    content += f"Danh m·ª•c: {product['category']}\n"
                    content += f"Gi√° g·ªëc: {product['price']:,.0f} VNƒê\n"
                    if product['sale_price']:
                        content += f"Gi√° khuy·∫øn m√£i: {product['sale_price']:,.0f} VNƒê\n"
                    content += f"T·ªìn kho: {product['stock']}\n"
                    content += f"M√¥ t·∫£: {product['description']}\n"
                    if product['variants']:
                        content += "Bi·∫øn th·ªÉ:\n"
                        for variant in product['variants'][:2]:  # Ch·ªâ hi·ªÉn th·ªã 2 variant ƒë·∫ßu
                            content += f"  - SKU: {variant['sku']}, Size: {variant['size']}, M√†u: {variant['color']}, T·ªìn kho: {variant['stock']}\n"
                    
                    db_results.append(Document(page_content=content, metadata={"source": "database_products"}))
            
            # C√¢u h·ªèi v·ªÅ ƒë∆°n h√†ng
            elif any(keyword in question_lower for keyword in ['ƒë∆°n h√†ng', 'order', 'mua', 'kh√°ch h√†ng']):
                # Extract m√£ ƒë∆°n h√†ng n·∫øu c√≥ (ORD...)
                order_code_match = re.search(r'ORD\d+', question.upper())
                search_term = order_code_match.group(0) if order_code_match else question
                
                print(f"üîç DEBUG: T√¨m ki·∫øm ƒë∆°n h√†ng v·ªõi t·ª´ kh√≥a: '{search_term}'")
                orders = get_order_info_from_db(engine, search_term)
                print(f"üîç DEBUG: T√¨m th·∫•y {len(orders)} ƒë∆°n h√†ng")
                for order in orders[:2]:  # Ch·ªâ l·∫•y 2 ƒë∆°n h√†ng ƒë·∫ßu
                    print(f"üîç DEBUG: ƒê∆°n h√†ng {order['order_number']}, tr·∫°ng th√°i: {order['status']}")
                    content = f"ƒê∆°n h√†ng: {order['order_number']}\n"
                    content += f"Kh√°ch h√†ng: {order['customer_name']}\n"
                    content += f"ƒêi·ªán tho·∫°i: {order['phone']}\n"
                    content += f"Tr·∫°ng th√°i: {order['status']}\n"
                    content += f"Ng√†y t·∫°o: {order['created_at']}\n"
                    if order['items']:
                        content += "S·∫£n ph·∫©m:\n"
                        for item in order['items']:
                            content += f"  - {item['product_name']}: {item['quantity']} x {item['price']:,.0f} = {item['subtotal']:,.0f} VNƒê\n"
                    
                    db_results.append(Document(page_content=content, metadata={"source": "database_orders"}))
        
        # 3. K·∫øt h·ª£p k·∫øt qu·∫£
        all_results = vector_results + db_results
        return all_results[:3]  # Gi·ªõi h·∫°n 3 k·∫øt qu·∫£

    # --- 6. T·∫†O PROMPT V√Ä CHAIN ---
    rag_template = """<s>[INST] B·∫°n l√† tr·ª£ l√Ω AI c·ªßa shop th·ªùi trang. Tr·∫£ l·ªùi CH√çNH X√ÅC d·ª±a tr√™n d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p.

QUY T·∫ÆC B·∫ÆT BU·ªòC:
1. CH·ªà s·ª≠ d·ª•ng th√¥ng tin t·ª´ "N·ªôi dung" b√™n d∆∞·ªõi
2. KH√îNG ƒë∆∞·ª£c t·ª± b·ªãa ho·∫∑c ƒëo√°n th√¥ng tin
3. N·∫øu kh√¥ng c√≥ ƒë·ªß th√¥ng tin: "Em kh√¥ng t√¨m th·∫•y th√¥ng tin v·ªÅ [n·ªôi dung] ·∫°"
4. X∆∞ng h√¥: t·ª± x∆∞ng "em", g·ªçi kh√°ch "anh/ch·ªã"
5. K·∫øt th√∫c: "Anh/Ch·ªã c√≥ c·∫ßn em t∆∞ v·∫•n th√™m g√¨ kh√¥ng ·∫°?"

N·ªôi dung (ƒê·ªåC K·ª∏ v√† S·ª¨ D·ª§NG):
{context}

C√¢u h·ªèi: {question}

H√£y tr·∫£ l·ªùi D·ª∞A TR√äN N·ªôi dung ph√≠a tr√™n, kh√¥ng ƒë∆∞·ª£c t·ª± b·ªãa: [/INST]
"""
    rag_prompt = PromptTemplate.from_template(rag_template)

    def format_docs(docs):
        return "\n\n---\n\n".join(doc.page_content for doc in docs)

    # T·∫°o chain v·ªõi c√∫ ph√°p t∆∞∆°ng th√≠ch
    def enhanced_context_retriever(inputs):
        """Retrieve v√† format context t·ª´ hybrid retriever."""
        question = inputs if isinstance(inputs, str) else inputs.get("question", "")
        docs = hybrid_retriever(question)
        return format_docs(docs)

    # RunnableLambda already imported at top
    
    rag_chain = (
        RunnableLambda(lambda x: {"context": enhanced_context_retriever(x), "question": x})
        | rag_prompt
        | llm
        | StrOutputParser()
    )
    print("‚úÖ Pipeline RAG v·ªõi database integration ho√†n ch·ªânh ƒë√£ s·∫µn s√†ng.")
    return rag_chain